{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18120,
     "status": "ok",
     "timestamp": 1690247230707,
     "user": {
      "displayName": "늘보곰",
      "userId": "01234514892132957537"
     },
     "user_tz": -540
    },
    "id": "jeaiTw1BlADA",
    "outputId": "35f5bf5a-2e82-42fa-a5b2-744524ff803f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2954,
     "status": "ok",
     "timestamp": 1690247319494,
     "user": {
      "displayName": "늘보곰",
      "userId": "01234514892132957537"
     },
     "user_tz": -540
    },
    "id": "2LOnsHFVlMtx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Flatten, Conv2D, Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJBcFK7YSt3A"
   },
   "source": [
    "# VAE 모델 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fqn3qHPBgheQ"
   },
   "source": [
    "##  Create a sampling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KdGvPjOtlRb2"
   },
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wOSBoWvglqL"
   },
   "source": [
    "## Build the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSRL7kjLaeAD"
   },
   "outputs": [],
   "source": [
    "latent_dim = 2\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuyovN9ahYA-"
   },
   "source": [
    "## Build the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dosC16jhhcJN"
   },
   "outputs": [],
   "source": [
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape((7, 7, 64))(x)\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "US3bqUFhhl4w"
   },
   "source": [
    "## Define VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96iWMJi8hEUR"
   },
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "      if isinstance(data, tuple):\n",
    "          data = data[0]\n",
    "\n",
    "      # 라벨 있는 데이터셋인 경우\n",
    "      if len(data.shape) == 4:\n",
    "          with tf.GradientTape() as tape:\n",
    "              z_mean, z_log_var, z = self.encoder(data)\n",
    "              reconstruction = self.decoder(z)\n",
    "              reconstruction_loss = tf.reduce_mean(\n",
    "                  keras.losses.binary_crossentropy(data, reconstruction)\n",
    "              )\n",
    "              kl_loss = -0.5 * tf.reduce_mean(\n",
    "                  1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "              )\n",
    "              total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "          gradients = tape.gradient(total_loss, self.trainable_variables)\n",
    "          self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "          return {\n",
    "              \"loss\": total_loss,\n",
    "              \"reconstruction_loss\": reconstruction_loss,\n",
    "              \"kl_loss\": kl_loss,\n",
    "          }\n",
    "\n",
    "      # 라벨 없는 데이터셋인 경우 (라벨이 없으므로 라벨 관련 손실 함수는 계산하지 않음)\n",
    "      else:\n",
    "          with tf.GradientTape() as tape:\n",
    "              z_mean, z_log_var, z = self.encoder(data)\n",
    "              reconstruction = self.decoder(z)\n",
    "              reconstruction_loss = tf.reduce_mean(\n",
    "                  keras.losses.binary_crossentropy(data, reconstruction)\n",
    "              )\n",
    "\n",
    "          gradients = tape.gradient(reconstruction_loss, self.trainable_variables)\n",
    "          self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "          return {\"loss\": reconstruction_loss}\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-NpVWGGGhoyN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQ_e9gzOZhOl"
   },
   "source": [
    "# 라벨링 된 데이터 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94mWGj9llOmB"
   },
   "outputs": [],
   "source": [
    "# 경로 안에 있는 모든 이미지를 데이터화하는 함수\n",
    "\n",
    "def preprocess_image(image_path, target_size=(28, 28)):\n",
    "    # 이미지 파일 경로 불러오기\n",
    "    image_list = glob(os.path.join(image_path, '*.png'))\n",
    "\n",
    "    # 빈 numpy 배열 생성\n",
    "    img_array = np.empty((len(image_list), *target_size, 1), dtype=np.uint8)\n",
    "\n",
    "    for i, img in enumerate(image_list):\n",
    "        image = Image.open(img)\n",
    "        image = image.resize(target_size)\n",
    "        img_array[i] = np.array(image)[:, :, np.newaxis]\n",
    "\n",
    "    return img_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-w8_--y9YGL_"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 정상 up 이미지 로드\n",
    "X_normal_up = preprocess_image('/content/drive/MyDrive/Project/good_up')\n",
    "# 정상 under 이미지 로드\n",
    "X_normal_under = preprocess_image('/content/drive/MyDrive/Project/good_under')\n",
    "# 비정상 up 이미지 로드\n",
    "X_abnormal_up = preprocess_image('/content/drive/MyDrive/Project/notgood_up')\n",
    "# 비정상 under 이미지 로드\n",
    "X_abnormal_under = preprocess_image('/content/drive/MyDrive/Project/notgood_under')\n",
    "\n",
    "# 각 클래스에 대한 클래스 레이블 생성\n",
    "# \"정상 up\" 클래스: 0\n",
    "# \"정상 under\" 클래스: 1\n",
    "# \"비정상 up\" 클래스: 2\n",
    "# \"비정상 under\" 클래스: 3\n",
    "y_normal_up = np.zeros(len(X_normal_up))\n",
    "y_normal_under = np.ones(len(X_normal_under))\n",
    "y_abnormal_up = np.full(len(X_abnormal_up), 2)\n",
    "y_abnormal_under = np.full(len(X_abnormal_under), 3)\n",
    "\n",
    "# 각 클래스별 데이터를 하나로 합치기\n",
    "# X_up은 \"정상 up\"과 \"비정상 up\" 데이터를 하나로 합침\n",
    "X_up = np.concatenate((X_normal_up, X_abnormal_up), axis=0)\n",
    "# X_under -> \"정상 under\"과 \"비정상 under\" 데이터를 하나로 합침\n",
    "X_under = np.concatenate((X_normal_under, X_abnormal_under), axis=0)\n",
    "# y_up -> \"정상 up\"과 \"비정상 up\"을 하나로 합쳐서 y_up 안에는 0과 1 두 개의 레이블이 존재\n",
    "y_up = np.concatenate((y_normal_up, y_abnormal_up), axis=0)\n",
    "# y_under -> \"정상 under\"과 \"비정상 under\"을 하나로 합쳐서 y_under 안에는 0과 1 두 개의 레이블이 존재\n",
    "y_under = np.concatenate((y_normal_under, y_abnormal_under), axis=0)\n",
    "\n",
    "# 각 데이터를 Train, Test 데이터로 나누기\n",
    "X_train_up, X_test_up, y_train_up, y_test_up = train_test_split(X_up, y_up, test_size=0.2, random_state=42, shuffle=False)\n",
    "X_train_under, X_test_under, y_train_under, y_test_under = train_test_split(X_under, y_under, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WuzD_R5VYSgG"
   },
   "outputs": [],
   "source": [
    "# 이미지 정규화\n",
    "X_train_up = X_train_up / 255\n",
    "X_test_up = X_test_up / 255\n",
    "\n",
    "X_train_under = X_train_under / 255\n",
    "X_test_under = X_test_under / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 299,
     "status": "ok",
     "timestamp": 1690214436314,
     "user": {
      "displayName": "늘보곰",
      "userId": "01234514892132957537"
     },
     "user_tz": -540
    },
    "id": "P57QqfUJoPGB",
    "outputId": "35c3f251-0b6a-46bb-b03c-8a86adb5a7e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train_up))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZc4LXn3jazz"
   },
   "source": [
    "### VAE 모델 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l_QcazpwZgCa"
   },
   "outputs": [],
   "source": [
    "vae = VAE(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaD4QVNQj3Us"
   },
   "source": [
    "### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmIZVyCZjgmt"
   },
   "outputs": [],
   "source": [
    "# Adam Optimizer와 MSE Loss 함수 사용\n",
    "vae.compile(optimizer = Adam(), loss = 'mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-M7EHxDj9Uv"
   },
   "source": [
    "### 라벨링 된 데이터 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZ-xIlEglQZE"
   },
   "outputs": [],
   "source": [
    "# X_train_up와 X_train_under를 하나의 데이터셋으로 합치기\n",
    "X_train_combined = np.concatenate((X_train_up, X_train_under), axis=0)\n",
    "X_train_combined = tf.convert_to_tensor(X_train_combined, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# tf.data.Dataset으로 변환\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(X_train_combined)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(X_train_combined)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1690214291433,
     "user": {
      "displayName": "늘보곰",
      "userId": "01234514892132957537"
     },
     "user_tz": -540
    },
    "id": "XJFfHCMtminA",
    "outputId": "bdd1a615-f152-4423-efba-1e03ebf14e2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'tensorflow.python.data.ops.batch_op._BatchDataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train_combined))\n",
    "print(type(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8787,
     "status": "ok",
     "timestamp": 1690215926844,
     "user": {
      "displayName": "늘보곰",
      "userId": "01234514892132957537"
     },
     "user_tz": -540
    },
    "id": "Zo5Hl0Snj8mG",
    "outputId": "f29978a3-e7ce-42a0-ab32-3cb03d19d99a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6/6 [==============================] - 2s 115ms/step - loss: 0.4396 - reconstruction_loss: 0.4440 - kl_loss: 3.5273e-06 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.4433 - reconstruction_loss: 0.4602 - kl_loss: 3.7466e-06 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.4409 - reconstruction_loss: 0.4520 - kl_loss: 1.3588e-05 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.4288 - reconstruction_loss: 0.4065 - kl_loss: 4.0191e-06 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 0.4294 - reconstruction_loss: 0.4085 - kl_loss: 2.1905e-06 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.4301 - reconstruction_loss: 0.4128 - kl_loss: 6.4905e-06 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.4385 - reconstruction_loss: 0.4467 - kl_loss: 3.1229e-06 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 1s 137ms/step - loss: 0.4418 - reconstruction_loss: 0.4584 - kl_loss: 3.6955e-06 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.4346 - reconstruction_loss: 0.4324 - kl_loss: 2.9121e-06 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.4346 - reconstruction_loss: 0.4333 - kl_loss: 1.1927e-05 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7876412d2020>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.fit(train_dataset, epochs=10, validation_data=(X_test_up, X_test_up))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eLPWWbBprZD"
   },
   "source": [
    "## 라벨 없는 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hi9QuBlTkUQH"
   },
   "outputs": [],
   "source": [
    "# Unlabeled 데이터\n",
    "X_unlabel = preprocess_image('/content/drive/MyDrive/Project/photo2')\n",
    "X_unlabel = X_unlabel / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvQ-pRwoqs7E"
   },
   "outputs": [],
   "source": [
    "# Unlabeled 데이터셋을 텐서로 변환\n",
    "X_unlabel = tf.convert_to_tensor(X_unlabel, dtype=tf.float32)\n",
    "\n",
    "# train_dataset에 라벨 없는 데이터셋 추가\n",
    "train_dataset_unlabel = tf.data.Dataset.from_tensor_slices(X_unlabel)\n",
    "train_dataset_unlabel = train_dataset_unlabel.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jd3w-DUNrdGO"
   },
   "outputs": [],
   "source": [
    "# 기존 라벨 있는 데이터셋과 라벨 없는 데이터셋을 합침\n",
    "train_dataset = tf.data.Dataset.zip((train_dataset, train_dataset_unlabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5450,
     "status": "ok",
     "timestamp": 1690215950205,
     "user": {
      "displayName": "늘보곰",
      "userId": "01234514892132957537"
     },
     "user_tz": -540
    },
    "id": "Q_fD76yWrhIi",
    "outputId": "68a6908d-b572-4d58-9da4-b9cecdf715e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6/6 [==============================] - 1s 64ms/step - loss: 0.4415 - reconstruction_loss: 0.4586 - kl_loss: 6.9197e-05 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.4375 - reconstruction_loss: 0.4430 - kl_loss: 6.8652e-05 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 0s 58ms/step - loss: 0.4335 - reconstruction_loss: 0.4249 - kl_loss: 1.2719e-05 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.4349 - reconstruction_loss: 0.4345 - kl_loss: 3.4709e-05 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.4349 - reconstruction_loss: 0.4335 - kl_loss: 1.1651e-05 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.4288 - reconstruction_loss: 0.4077 - kl_loss: 1.5389e-05 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 0.4343 - reconstruction_loss: 0.4294 - kl_loss: 3.9213e-05 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.4328 - reconstruction_loss: 0.4261 - kl_loss: 2.5868e-05 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.4297 - reconstruction_loss: 0.4131 - kl_loss: 7.1611e-06 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 1s 96ms/step - loss: 0.4383 - reconstruction_loss: 0.4470 - kl_loss: 1.0878e-06 - val_total_loss: 0.0000e+00 - val_reconstruction_loss: 0.0000e+00 - val_kl_loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7876413aba00>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VAE 모델 훈련\n",
    "vae.fit(train_dataset, epochs=10, validation_data=(X_test_up, X_test_up))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_162_r4br31r"
   },
   "source": [
    "# 분류할 이미지 넣고 분류시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0dN8sL03tfRn"
   },
   "outputs": [],
   "source": [
    "real_test = preprocess_image('/content/drive/MyDrive/Project/photo1')\n",
    "real_test = real_test / 255\n",
    "# real_test데이터셋을 텐서로 변환\n",
    "real_test = tf.convert_to_tensor(real_test, dtype=tf.float32)\n",
    "\n",
    "# train_dataset에 라벨 없는 데이터셋 추가\n",
    "real_test_dataset = tf.data.Dataset.from_tensor_slices(real_test)\n",
    "real_test_dataset = real_test_dataset.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "executionInfo": {
     "elapsed": 1329314,
     "status": "error",
     "timestamp": 1690218523098,
     "user": {
      "displayName": "늘보곰",
      "userId": "01234514892132957537"
     },
     "user_tz": -540
    },
    "id": "zNI4MTajriS6",
    "outputId": "0027084c-22db-4bbf-9f7d-f523440b1039"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 7ms/step\n",
      "36/36 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-50d230910863>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# 이미지 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mresult_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'results_{num_files + 1}.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             \u001b[0msave_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2241\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m             \u001b[0;31m# do what we can to clean up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/PngImagePlugin.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"flush\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# VAE 모델의 잠재 특성을 추출하는 인코더 모델 정의\n",
    "encoder_model = keras.Model(inputs=vae.encoder.input, outputs=vae.encoder.get_layer('sampling').output)\n",
    "\n",
    "# 테스트 데이터에 대한 잠재 특성 추출\n",
    "latent_test = encoder_model.predict(real_test)\n",
    "\n",
    "# 분류 모델 정의\n",
    "classification_model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(latent_dim,)),  # 잠재 특성의 차원에 맞게 입력 레이어 설정\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(4, activation='softmax')  # num_classes는 분류할 클래스 수\n",
    "])\n",
    "\n",
    "# 분류 모델 컴파일\n",
    "classification_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 테스트 데이터에 대한 분류 결과 예측\n",
    "predictions = classification_model.predict(latent_test)\n",
    "\n",
    "# real_test를 다시 NumPy 배열로 변환\n",
    "real_test_np = real_test.numpy()\n",
    "\n",
    "# 분류 결과에 따라 이미지 저장\n",
    "for i in range(len(real_test_np)):\n",
    "    class_index = np.argmax(predictions[i])\n",
    "\n",
    "    # 분류 결과에 따라 이미지 저장\n",
    "    if class_index == 0:\n",
    "        save_path = '/content/drive/MyDrive/Project/results/up_good'\n",
    "    elif class_index == 1:\n",
    "        save_path = '/content/drive/MyDrive/Project/results/under_good'\n",
    "    elif class_index == 2:\n",
    "        save_path = '/content/drive/MyDrive/Project/results/up_notgood'\n",
    "    else:\n",
    "        save_path = '/content/drive/MyDrive/Project/results/under_notgood'\n",
    "\n",
    "    # 저장할 폴더 내 이미지 파일들 개수 확인\n",
    "    file_list = os.listdir(save_path)\n",
    "    num_files = len(file_list)\n",
    "\n",
    "    # 이미지 저장을 위해 다시 NumPy 배열로 변환하고 모드를 'L'로 변경\n",
    "    result_image = np.array(real_test_np[i][0] * 255, dtype=np.uint8)\n",
    "    result_image = Image.fromarray(result_image, mode='L')\n",
    "\n",
    "    # 이미지 저장\n",
    "    result_image.save(os.path.join(save_path, f'results_{num_files + 1}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-k42UfwtEdm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPyvxr19JLmCdpe2Zq2hQz3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
